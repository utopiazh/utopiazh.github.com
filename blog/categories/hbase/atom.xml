<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hbase, | Coding For Fun]]></title>
  <link href="http://utopiazh.github.com/blog/categories/hbase/atom.xml" rel="self"/>
  <link href="http://utopiazh.github.com/"/>
  <updated>2013-03-10T09:47:02+08:00</updated>
  <id>http://utopiazh.github.com/</id>
  <author>
    <name><![CDATA[Hang Zhou]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Multi-tenant of HBase in FaceBook]]></title>
    <link href="http://utopiazh.github.com/blog/2013/03/10/multi-tenancies-of-hbase-in-facebook/"/>
    <updated>2013-03-10T09:24:00+08:00</updated>
    <id>http://utopiazh.github.com/blog/2013/03/10/multi-tenancies-of-hbase-in-facebook</id>
    <content type="html"><![CDATA[<h1>Overview</h1>

<p>Watched the video for 2 times to get the main point, the highlights learned from this video is:
+ Use unique message id as ts to store into hbase
+ Use monitoring/reporting tools to know what exactly the access pattern is, while the app developers don't know either
+ Use underlying hashout solution to isolate traffic, instead of an optimistic solution of using table names.</p>

<h1>HBase adoption</h1>

<h2>Messages:</h2>

<p><em>Requirements:</em>
- Very high volume (9B+ msg, 90B r+w ops per day), 4.5 PB no compressed.
- Elasticity &amp; auto-failover
- Strong consistency within a data centeer
- import large data set (old data)</p>

<p><em>Schema:</em>
- 3 CF (actions, snapshot (cache), keywords (search))
- Use message id as timestamp, which leverage the 3rd dimension.
- Fast iteration on schema 
    - Action logs are single source of truth
    - Most data during schema iteration from snapshot  and keywords
    - Custom backup solution (action logs), which could be replayed on schema evolving.</p>

<h2>ODS (Operational Data Store)</h2>

<ul>
<li>Schema: 3CFs, raw, hour, days</li>
<li>MR job to move data to buckets</li>
<li>Compaction tricks (leverage the time series to pick the right HFile to achieve more spatial locality)</li>
<li>Separating HBase clusters and its backing HDFS clusters proved not feasible</li>
<li>HA is essential (Master/Master cluster replication with MR jobs)</li>
</ul>


<h1>HBase best practices</h1>

<p><em>Process:</em>
Select the right apps: no five nines, etc.</p>

<p>Shadow testing before going production (real traffic goes in and out without noticed by users).</p>

<p><em>Schema Design</em>
One CF or Multiple CF: common issue is putting all data into one CF, so get poor spatial locality </p>

<p>New table or New CF: New CF provides better locality, e.g. users message and search, the best user experience is that all users data is complete available or down</p>

<p><em>Lessons:</em>
Need graph/reporting monitoring to provide insights
Constant upgrade (facebook is running HBase 0.89 with back-ported patches)</p>

<h1>Multi-Tenants</h1>

<p><em>optimistic multi-tenancies</em>
- let users do the right things
- lack of table naming rules</p>

<p><em>Hashout:</em>
- Was for Mysql, ported to support HBase
- review every app
- block abusive users, promote heave users
- users must provide app id and key, system app id to specific shard
- backed reading ops with memcached
- intensive MR job on separate cluster</p>

<h1>Reference</h1>

<ul>
<li><a href="vimeo.com/44715953" title="Multi-tenant HBase Solutions at Facebook">Nicolas Spiegelberg - Multi-tenant HBase Solutions at Facebook</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
